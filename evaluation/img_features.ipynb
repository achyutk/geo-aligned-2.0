{"cells":[{"cell_type":"code","execution_count":null,"id":"447c2ee3","metadata":{"id":"447c2ee3"},"outputs":[],"source":["# !pip install -U torch torchvision"]},{"cell_type":"code","execution_count":null,"id":"509507da","metadata":{"id":"509507da"},"outputs":[],"source":["# !pip install -U git+https://github.com/openai/CLIP.git"]},{"cell_type":"code","execution_count":null,"id":"d40a8850","metadata":{"id":"d40a8850"},"outputs":[],"source":["import torch\n","import clip\n","from PIL import Image\n","import os\n","import numpy as np\n","import pandas as pd\n","import csv"]},{"cell_type":"code","execution_count":null,"id":"5bcc07c5","metadata":{"id":"5bcc07c5"},"outputs":[],"source":["def get_clip_score_img_features(model, preprocess,image_path):\n","    image = Image.open(image_path)\n","\n","    # Preprocess the image and tokenize the text\n","    image_input = preprocess(image).unsqueeze(0)\n","\n","    # Move the inputs to GPU if available\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    image_input = image_input.to(device)\n","    model = model.to(device)\n","\n","    # Generate embeddings for the image and text\n","    with torch.no_grad():\n","        image_features = model.encode_image(image_input)\n","\n","    # Normalize the features\n","    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n","\n","    return image_features"]},{"cell_type":"code","execution_count":null,"id":"e965e274","metadata":{"id":"e965e274"},"outputs":[],"source":["# Load the pre-trained CLIP model and the image\n","model, preprocess = clip.load('ViT-B/32')"]},{"cell_type":"code","execution_count":null,"id":"69de0260","metadata":{"scrolled":true,"id":"69de0260"},"outputs":[],"source":["features=[] # List to store all the features\n","country=[] # List to store names of countries\n","path='image_data'\n","\n","folders = os.listdir(path) # List to store country names\n","\n","# Iterating over folders of countries\n","for i in folders:\n","    files= os.listdir(path+'/'+i)\n","\n","    # Iterating over images\n","    for j in files:\n","        image_path = path + '/' + i + '/' + j # path of image\n","\n","        # Extracting features\n","        img_feature = get_clip_score_img_features(model,preprocess,image_path)\n","\n","        # Appending values\n","        features.append(img_feature)\n","        country.append(i.split('_')[0])"]},{"cell_type":"code","execution_count":null,"id":"5a2fd0da","metadata":{"id":"5a2fd0da"},"outputs":[],"source":["# storing the list of tensor the features in csv file\n","torch.save(features, 'eval_img_features.pt')"]},{"cell_type":"code","execution_count":null,"id":"aef5571a","metadata":{"id":"aef5571a"},"outputs":[],"source":["# Save the list of strings to a text file\n","with open('eval_img_features_country_list.txt', 'w') as file:\n","    for string in country:\n","        file.write(string + '\\n')"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}